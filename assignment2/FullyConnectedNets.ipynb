{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fully-Connected Neural Nets\n",
    "In the previous homework you implemented a fully-connected two-layer neural network on CIFAR-10. The implementation was simple but not very modular since the loss and gradient were computed in a single monolithic function. This is manageable for a simple two-layer network, but would become impractical as we move to bigger models. Ideally we want to build networks using a more modular design so that we can implement different layer types in isolation and then snap them together into models with different architectures.\n",
    "\n",
    "In this exercise we will implement fully-connected networks using a more modular approach. For each layer we will implement a `forward` and a `backward` function. The `forward` function will receive inputs, weights, and other parameters and will return both an output and a `cache` object storing data needed for the backward pass, like this:\n",
    "\n",
    "```python\n",
    "def layer_forward(x, w):\n",
    "  \"\"\" Receive inputs x and weights w \"\"\"\n",
    "  # Do some computations ...\n",
    "  z = # ... some intermediate value\n",
    "  # Do some more computations ...\n",
    "  out = # the output\n",
    "   \n",
    "  cache = (x, w, z, out) # Values we need to compute gradients\n",
    "   \n",
    "  return out, cache\n",
    "```\n",
    "\n",
    "The backward pass will receive upstream derivatives and the `cache` object, and will return gradients with respect to the inputs and weights, like this:\n",
    "\n",
    "```python\n",
    "def layer_backward(dout, cache):\n",
    "  \"\"\"\n",
    "  Receive derivative of loss with respect to outputs and cache,\n",
    "  and compute derivative with respect to inputs.\n",
    "  \"\"\"\n",
    "  # Unpack cache values\n",
    "  x, w, z, out = cache\n",
    "  \n",
    "  # Use values in cache to compute derivatives\n",
    "  dx = # Derivative of loss with respect to x\n",
    "  dw = # Derivative of loss with respect to w\n",
    "  \n",
    "  return dx, dw\n",
    "```\n",
    "\n",
    "After implementing a bunch of layers this way, we will be able to easily combine them to build classifiers with different architectures.\n",
    "\n",
    "In addition to implementing fully-connected networks of arbitrary depth, we will also explore different update rules for optimization, and introduce Dropout as a regularizer and Batch Normalization as a tool to more efficiently optimize deep networks.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# As usual, a bit of setup\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from cs231n.classifiers.fc_net import *\n",
    "from cs231n.data_utils import get_CIFAR10_data\n",
    "from cs231n.gradient_check import eval_numerical_gradient, eval_numerical_gradient_array\n",
    "from cs231n.solver import Solver\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "def rel_error(x, y):\n",
    "  \"\"\" returns relative error \"\"\"\n",
    "  return np.max(np.abs(x - y) / (np.maximum(1e-8, np.abs(x) + np.abs(y))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_val:  (1000, 3, 32, 32)\n",
      "X_train:  (49000, 3, 32, 32)\n",
      "X_test:  (1000, 3, 32, 32)\n",
      "y_val:  (1000,)\n",
      "y_train:  (49000,)\n",
      "y_test:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Load the (preprocessed) CIFAR10 data.\n",
    "\n",
    "data = get_CIFAR10_data()\n",
    "for k, v in data.iteritems():\n",
    "  print '%s: ' % k, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: foward\n",
    "Open the file `cs231n/layers.py` and implement the `affine_forward` function.\n",
    "\n",
    "Once you are done you can test your implementaion by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_forward function:\n",
      "difference:  9.76984772881e-10\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_forward function\n",
    "\n",
    "num_inputs = 2\n",
    "input_shape = (4, 5, 6)\n",
    "output_dim = 3\n",
    "\n",
    "input_size = num_inputs * np.prod(input_shape)\n",
    "weight_size = output_dim * np.prod(input_shape)\n",
    "\n",
    "x = np.linspace(-0.1, 0.5, num=input_size).reshape(num_inputs, *input_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=weight_size).reshape(np.prod(input_shape), output_dim)\n",
    "b = np.linspace(-0.3, 0.1, num=output_dim)\n",
    "\n",
    "out, _ = affine_forward(x, w, b)\n",
    "correct_out = np.array([[ 1.49834967,  1.70660132,  1.91485297],\n",
    "                        [ 3.25553199,  3.5141327,   3.77273342]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-9.\n",
    "print 'Testing affine_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Affine layer: backward\n",
    "Now implement the `affine_backward` function and test your implementation using numeric gradient checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Testing affine_backward function:\n",
      "dx error:  2.00146088489e-10\n",
      "dw error:  1.13556015747e-10\n",
      "db error:  1.52456243688e-11\n"
     ]
    }
   ],
   "source": [
    "# Test the affine_backward function\n",
    "\n",
    "x = np.random.randn(10, 2, 3)\n",
    "w = np.random.randn(6, 5)\n",
    "b = np.random.randn(5)\n",
    "dout = np.random.randn(10, 5)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "_, cache = affine_forward(x, w, b)\n",
    "dx, dw, db = affine_backward(dout, cache)\n",
    "\n",
    "# The error should be around 1e-10\n",
    "print 'Testing affine_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: forward\n",
    "Implement the forward pass for the ReLU activation function in the `relu_forward` function and test your implementation using the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_forward function:\n",
      "difference:  4.99999979802e-08\n"
     ]
    }
   ],
   "source": [
    "# Test the relu_forward function\n",
    "\n",
    "x = np.linspace(-0.5, 0.5, num=12).reshape(3, 4)\n",
    "\n",
    "out, _ = relu_forward(x)\n",
    "correct_out = np.array([[ 0.,          0.,          0.,          0.,        ],\n",
    "                        [ 0.,          0.,          0.04545455,  0.13636364,],\n",
    "                        [ 0.22727273,  0.31818182,  0.40909091,  0.5,       ]])\n",
    "\n",
    "# Compare your output with ours. The error should be around 1e-8\n",
    "print 'Testing relu_forward function:'\n",
    "print 'difference: ', rel_error(out, correct_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ReLU layer: backward\n",
    "Now implement the backward pass for the ReLU activation function in the `relu_backward` function and test your implementation using numeric gradient checking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing relu_backward function:\n",
      "dx error:  3.27562466413e-12\n"
     ]
    }
   ],
   "source": [
    "x = np.random.randn(10, 10)\n",
    "dout = np.random.randn(*x.shape)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: relu_forward(x)[0], x, dout)\n",
    "\n",
    "_, cache = relu_forward(x)\n",
    "dx = relu_backward(dout, cache)\n",
    "\n",
    "\n",
    "# The error should be around 1e-12\n",
    "print 'Testing relu_backward function:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Sandwich\" layers\n",
    "There are some common patterns of layers that are frequently used in neural nets. For example, affine layers are frequently followed by a ReLU nonlinearity. To make these common patterns easy, we define several convenience layers in the file `cs231n/layer_utils.py`.\n",
    "\n",
    "For now take a look at the `affine_relu_forward` and `affine_relu_backward` functions, and run the following to numerically gradient check the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing affine_relu_forward:\n",
      "dx error:  6.02739994389e-11\n",
      "dw error:  1.69494613061e-10\n",
      "db error:  7.82666595909e-12\n"
     ]
    }
   ],
   "source": [
    "from cs231n.layer_utils import affine_relu_forward, affine_relu_backward\n",
    "\n",
    "x = np.random.randn(2, 3, 4)\n",
    "w = np.random.randn(12, 10)\n",
    "b = np.random.randn(10)\n",
    "dout = np.random.randn(2, 10)\n",
    "\n",
    "out, cache = affine_relu_forward(x, w, b)\n",
    "dx, dw, db = affine_relu_backward(dout, cache)\n",
    "\n",
    "dx_num = eval_numerical_gradient_array(lambda x: affine_relu_forward(x, w, b)[0], x, dout)\n",
    "dw_num = eval_numerical_gradient_array(lambda w: affine_relu_forward(x, w, b)[0], w, dout)\n",
    "db_num = eval_numerical_gradient_array(lambda b: affine_relu_forward(x, w, b)[0], b, dout)\n",
    "\n",
    "print 'Testing affine_relu_forward:'\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "print 'dw error: ', rel_error(dw_num, dw)\n",
    "print 'db error: ', rel_error(db_num, db)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss layers: Softmax and SVM\n",
    "You implemented these loss functions in the last assignment, so we'll give them to you for free here. You should still make sure you understand how they work by looking at the implementations in `cs231n/layers.py`.\n",
    "\n",
    "You can make sure that the implementations are correct by running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing svm_loss:\n",
      "loss:  8.9996606329\n",
      "dx error:  8.18289447289e-10\n",
      "\n",
      "Testing softmax_loss:\n",
      "loss:  2.30255162623\n",
      "dx error:  7.82805290152e-09\n"
     ]
    }
   ],
   "source": [
    "num_classes, num_inputs = 10, 50\n",
    "x = 0.001 * np.random.randn(num_inputs, num_classes)\n",
    "y = np.random.randint(num_classes, size=num_inputs)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: svm_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = svm_loss(x, y)\n",
    "\n",
    "# Test svm_loss function. Loss should be around 9 and dx error should be 1e-9\n",
    "print 'Testing svm_loss:'\n",
    "print 'loss: ', loss\n",
    "print 'dx error: ', rel_error(dx_num, dx)\n",
    "\n",
    "dx_num = eval_numerical_gradient(lambda x: softmax_loss(x, y)[0], x, verbose=False)\n",
    "loss, dx = softmax_loss(x, y)\n",
    "\n",
    "# Test softmax_loss function. Loss should be 2.3 and dx error should be 1e-8\n",
    "print '\\nTesting softmax_loss:'\n",
    "print 'loss: ', loss\n",
    "print 'dx error: ', rel_error(dx_num, dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-layer network\n",
    "In the previous assignment you implemented a two-layer neural network in a single monolithic class. Now that you have implemented modular versions of the necessary layers, you will reimplement the two layer network using these modular implementations.\n",
    "\n",
    "Open the file `cs231n/classifiers/fc_net.py` and complete the implementation of the `TwoLayerNet` class. This class will serve as a model for the other networks you will implement in this assignment, so read through it to make sure you understand the API. You can run the cell below to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing initialization ... \n",
      "Testing test-time forward pass ... \n",
      "Testing training loss (no regularization)\n",
      "Running numeric gradient check with reg =  0.0\n",
      "W1 relative error: 1.52e-08\n",
      "W2 relative error: 3.48e-10\n",
      "b1 relative error: 6.55e-09\n",
      "b2 relative error: 4.33e-10\n",
      "Running numeric gradient check with reg =  0.7\n",
      "W1 relative error: 8.18e-07\n",
      "W2 relative error: 2.85e-08\n",
      "b1 relative error: 1.09e-09\n",
      "b2 relative error: 9.09e-10\n"
     ]
    }
   ],
   "source": [
    "N, D, H, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=N)\n",
    "\n",
    "std = 1e-2\n",
    "model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)\n",
    "\n",
    "print 'Testing initialization ... '\n",
    "W1_std = abs(model.params['W1'].std() - std)\n",
    "b1 = model.params['b1']\n",
    "W2_std = abs(model.params['W2'].std() - std)\n",
    "b2 = model.params['b2']\n",
    "assert W1_std < std / 10, 'First layer weights do not seem right'\n",
    "assert np.all(b1 == 0), 'First layer biases do not seem right'\n",
    "assert W2_std < std / 10, 'Second layer weights do not seem right'\n",
    "assert np.all(b2 == 0), 'Second layer biases do not seem right'\n",
    "\n",
    "print 'Testing test-time forward pass ... '\n",
    "model.params['W1'] = np.linspace(-0.7, 0.3, num=D*H).reshape(D, H)\n",
    "model.params['b1'] = np.linspace(-0.1, 0.9, num=H)\n",
    "model.params['W2'] = np.linspace(-0.3, 0.4, num=H*C).reshape(H, C)\n",
    "model.params['b2'] = np.linspace(-0.9, 0.1, num=C)\n",
    "X = np.linspace(-5.5, 4.5, num=N*D).reshape(D, N).T\n",
    "scores = model.loss(X)\n",
    "correct_scores = np.asarray(\n",
    "  [[11.53165108,  12.2917344,   13.05181771,  13.81190102,  14.57198434, 15.33206765,  16.09215096],\n",
    "   [12.05769098,  12.74614105,  13.43459113,  14.1230412,   14.81149128, 15.49994135,  16.18839143],\n",
    "   [12.58373087,  13.20054771,  13.81736455,  14.43418138,  15.05099822, 15.66781506,  16.2846319 ]])\n",
    "scores_diff = np.abs(scores - correct_scores).sum()\n",
    "assert scores_diff < 1e-6, 'Problem with test-time forward pass'\n",
    "\n",
    "print 'Testing training loss (no regularization)'\n",
    "y = np.asarray([0, 5, 1])\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 3.4702243556\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with training-time loss'\n",
    "\n",
    "model.reg = 1.0\n",
    "loss, grads = model.loss(X, y)\n",
    "correct_loss = 26.5948426952\n",
    "assert abs(loss - correct_loss) < 1e-10, 'Problem with regularization loss'\n",
    "\n",
    "for reg in [0.0, 0.7]:\n",
    "  print 'Running numeric gradient check with reg = ', reg\n",
    "  model.reg = reg\n",
    "  loss, grads = model.loss(X, y)\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False)\n",
    "    print '%s relative error: %.2e' % (name, rel_error(grad_num, grads[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solver\n",
    "In the previous assignment, the logic for training models was coupled to the models themselves. Following a more modular design, for this assignment we have split the logic for training models into a separate class.\n",
    "\n",
    "Open the file `cs231n/solver.py` and read through it to familiarize yourself with the API. After doing so, use a `Solver` instance to train a `TwoLayerNet` that achieves at least `50%` accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3072)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3072)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3072)\n",
      "Test labels shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "from cs231n.data_utils import load_CIFAR10\n",
    "\n",
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the two-layer neural net classifier. These are the same steps as\n",
    "    we used for the SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "        \n",
    "    # Subsample the data\n",
    "    mask = range(num_training, num_training + num_validation)\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = range(num_training)\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = range(num_test)\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "\n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "\n",
    "    # Reshape data to rows\n",
    "    X_train = X_train.reshape(num_training, -1)\n",
    "    X_val = X_val.reshape(num_validation, -1)\n",
    "    X_test = X_test.reshape(num_test, -1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()\n",
    "print 'Train data shape: ', X_train.shape\n",
    "print 'Train labels shape: ', y_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Validation labels shape: ', y_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'Test labels shape: ', y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 4900) loss: 2.304953\n",
      "(Epoch 0 / 10) train acc: 0.152000; val_acc: 0.133000\n",
      "(Iteration 101 / 4900) loss: 1.799312\n",
      "(Iteration 201 / 4900) loss: 1.562322\n",
      "(Iteration 301 / 4900) loss: 1.778925\n",
      "(Iteration 401 / 4900) loss: 1.469789\n",
      "(Epoch 1 / 10) train acc: 0.465000; val_acc: 0.454000\n",
      "(Iteration 501 / 4900) loss: 1.665802\n",
      "(Iteration 601 / 4900) loss: 1.534972\n",
      "(Iteration 701 / 4900) loss: 1.595796\n",
      "(Iteration 801 / 4900) loss: 1.464106\n",
      "(Iteration 901 / 4900) loss: 1.233353\n",
      "(Epoch 2 / 10) train acc: 0.468000; val_acc: 0.471000\n",
      "(Iteration 1001 / 4900) loss: 1.302754\n",
      "(Iteration 1101 / 4900) loss: 1.315383\n",
      "(Iteration 1201 / 4900) loss: 1.510314\n",
      "(Iteration 1301 / 4900) loss: 1.499622\n",
      "(Iteration 1401 / 4900) loss: 1.538807\n",
      "(Epoch 3 / 10) train acc: 0.508000; val_acc: 0.482000\n",
      "(Iteration 1501 / 4900) loss: 1.559077\n",
      "(Iteration 1601 / 4900) loss: 1.372663\n",
      "(Iteration 1701 / 4900) loss: 1.258930\n",
      "(Iteration 1801 / 4900) loss: 1.383759\n",
      "(Iteration 1901 / 4900) loss: 1.474047\n",
      "(Epoch 4 / 10) train acc: 0.522000; val_acc: 0.452000\n",
      "(Iteration 2001 / 4900) loss: 1.217235\n",
      "(Iteration 2101 / 4900) loss: 1.348661\n",
      "(Iteration 2201 / 4900) loss: 1.340865\n",
      "(Iteration 2301 / 4900) loss: 1.362896\n",
      "(Iteration 2401 / 4900) loss: 1.603604\n",
      "(Epoch 5 / 10) train acc: 0.524000; val_acc: 0.473000\n",
      "(Iteration 2501 / 4900) loss: 1.339211\n",
      "(Iteration 2601 / 4900) loss: 1.354331\n",
      "(Iteration 2701 / 4900) loss: 1.101136\n",
      "(Iteration 2801 / 4900) loss: 1.235553\n",
      "(Iteration 2901 / 4900) loss: 1.230181\n",
      "(Epoch 6 / 10) train acc: 0.554000; val_acc: 0.499000\n",
      "(Iteration 3001 / 4900) loss: 1.324527\n",
      "(Iteration 3101 / 4900) loss: 1.196088\n",
      "(Iteration 3201 / 4900) loss: 1.379416\n",
      "(Iteration 3301 / 4900) loss: 1.197784\n",
      "(Iteration 3401 / 4900) loss: 1.178297\n",
      "(Epoch 7 / 10) train acc: 0.541000; val_acc: 0.518000\n",
      "(Iteration 3501 / 4900) loss: 1.325389\n",
      "(Iteration 3601 / 4900) loss: 1.247800\n",
      "(Iteration 3701 / 4900) loss: 1.250214\n",
      "(Iteration 3801 / 4900) loss: 1.089286\n",
      "(Iteration 3901 / 4900) loss: 1.280348\n",
      "(Epoch 8 / 10) train acc: 0.601000; val_acc: 0.512000\n",
      "(Iteration 4001 / 4900) loss: 1.261733\n",
      "(Iteration 4101 / 4900) loss: 1.044192\n",
      "(Iteration 4201 / 4900) loss: 1.274255\n",
      "(Iteration 4301 / 4900) loss: 1.242433\n",
      "(Iteration 4401 / 4900) loss: 1.185239\n",
      "(Epoch 9 / 10) train acc: 0.593000; val_acc: 0.524000\n",
      "(Iteration 4501 / 4900) loss: 1.090921\n",
      "(Iteration 4601 / 4900) loss: 1.348864\n",
      "(Iteration 4701 / 4900) loss: 1.372461\n",
      "(Iteration 4801 / 4900) loss: 1.165237\n",
      "(Epoch 10 / 10) train acc: 0.598000; val_acc: 0.501000\n",
      "best accuracy: 0.524\n"
     ]
    }
   ],
   "source": [
    "model = TwoLayerNet()\n",
    "solver = None\n",
    "\n",
    "##############################################################################\n",
    "# TODO: Use a Solver instance to train a TwoLayerNet that achieves at least  #\n",
    "# 50% accuracy on the validation set.                                        #\n",
    "##############################################################################\n",
    "\n",
    "\n",
    "data = {\n",
    "    'X_train': X_train, # training data\n",
    "    'y_train': y_train, # training labels\n",
    "    'X_val': X_val, # validation data\n",
    "    'y_val': y_val # validation labels\n",
    "}\n",
    "\n",
    "solver = Solver(model, data,\n",
    "                  update_rule='sgd',\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-3,\n",
    "                  },\n",
    "                  lr_decay=0.95,\n",
    "                  num_epochs=10, batch_size=100,\n",
    "                  print_every=100)\n",
    "\n",
    "solver.train()\n",
    "\n",
    "print 'best accuracy:', solver.best_val_acc\n",
    "##############################################################################\n",
    "#                             END OF YOUR CODE                               #\n",
    "##############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA20AAALJCAYAAAAnCMuGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d8d38d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to visualize training loss and train / val accuracy\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.title('Accuracy')\n",
    "plt.plot(solver.train_acc_history, '-o', label='train')\n",
    "plt.plot(solver.val_acc_history, '-o', label='val')\n",
    "plt.plot([0.5] * len(solver.val_acc_history), 'k--')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer network\n",
    "Next you will implement a fully-connected network with an arbitrary number of hidden layers.\n",
    "\n",
    "Read through the `FullyConnectedNet` class in the file `cs231n/classifiers/fc_net.py`.\n",
    "\n",
    "Implement the initialization, the forward pass, and the backward pass. For the moment don't worry about implementing dropout or batch normalization; we will add those features soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial loss and gradient check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, run the following to check the initial loss and to gradient check the network both with and without regularization. Do the initial losses seem reasonable?\n",
    "\n",
    "For gradient checking, you should expect to see errors around 1e-6 or less."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "Initial loss:  0.0\n",
      "Running check with reg =  3.14\n",
      "Initial loss:  0.0\n"
     ]
    }
   ],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print 'Running check with reg = ', reg\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  loss, grads = model.loss(X, y)\n",
    "  print 'Initial loss: ', loss\n",
    "\n",
    "  for name in sorted(grads):\n",
    "    f = lambda _: model.loss(X, y)[0]\n",
    "    grad_num = eval_numerical_gradient(f, model.params[name], verbose=False, h=1e-5)\n",
    "    print '%s relative error: %.2e' % (name, rel_error(grad_num, grads[name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running check with reg =  0\n",
      "(30, 20)\n",
      "(2, 15)\n",
      "Running check with reg =  3.14\n",
      "(30, 20)\n",
      "(2, 15)\n"
     ]
    }
   ],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "for reg in [0, 3.14]:\n",
    "  print 'Running check with reg = ', reg\n",
    "  model = FullyConnectedNet([H1, H2], input_dim=D, num_classes=C,\n",
    "                            reg=reg, weight_scale=5e-2, dtype=np.float64)\n",
    "\n",
    "  model.loss(X, y)\n",
    "  print 'Initial loss: ', loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, D, H1, H2, C = 2, 15, 20, 30, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "input_dim=D\n",
    "hidden_dims=[H1, H2]\n",
    "weight_scale=5e-2\n",
    "params = {}\n",
    "W = {}\n",
    "b = {}\n",
    "Z = {}\n",
    "\n",
    "    \n",
    "for reg in [0, 3.14]:\n",
    "\n",
    "    ############################################initializing parameters\n",
    "    params['W1'] = weight_scale * np.random.randn(input_dim, hidden_dims[0])\n",
    "    params['b1'] = np.zeros(hidden_dims[0])\n",
    "\n",
    "    for i in range(1, len(hidden_dims)):\n",
    "\n",
    "        params['W' + str(i+1)] = weight_scale * np.random.randn(hidden_dims[i-1], hidden_dims[i])    \n",
    "        params['b' + str(i+1)] = np.zeros(hidden_dims[i])\n",
    "\n",
    "    ################################################forward propagation\n",
    "    W[0] = params['W1']\n",
    "    b[0] = params['b1']\n",
    "    Z[0] = np.maximum(0, np.dot(X, W[0]) + b[0])\n",
    "\n",
    "    for i in range(1, len(hidden_dims)):\n",
    "\n",
    "        W[i] = params['W'+str(i+1)]\n",
    "        b[i] = params['b'+str(i+1)]\n",
    "\n",
    "        Z[i] = np.maximum(0, np.dot(Z[i-1], W[i]) + b[i])\n",
    "\n",
    "\n",
    "    scores = Z[len(hidden_dims)-1]\n",
    "\n",
    "    ################################################back propagation\n",
    "    loss, grads = 0.0, {}\n",
    "    delta = {}\n",
    "    dW = {}\n",
    "    \n",
    "    # get unnormalized probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    # normalize them for each example\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    \n",
    "\n",
    "    correct_logprobs = -np.log(probs[range(X.shape[0]),y])\n",
    "    \n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    data_loss = np.sum(correct_logprobs)/X.shape[0]\n",
    "    \n",
    "    reg_loss = 0\n",
    "    for i in range(1, len(hidden_dims)):\n",
    "        reg_loss += np.sum(W[i]*W[i])\n",
    "    \n",
    "    reg_loss *= 0.5*reg\n",
    "    \n",
    "    loss = data_loss + reg_loss\n",
    "    \n",
    "    probs_1 = probs\n",
    "    probs_1[np.arange(X.shape[0]), y] -= 1\n",
    "    \n",
    "    delta[len(hidden_dims)] = probs_1\n",
    "    delta[len(hidden_dims)] /= X.shape[0]\n",
    "\n",
    "    dW[len(hidden_dims)] = np.dot(Z[len(hidden_dims)-1].T, delta[len(hidden_dims)])\n",
    "\n",
    "    #dW[len(hidden_dims)] += reg * W[len(hidden_dims)]\n",
    "    #db[len(hidden_dims)] = np.sum(delta[len(hidden_dims)], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    grads['W2'] = dW2\n",
    "    grads['b2'] = db2\n",
    "        \n",
    "    delta1 = np.dot(delta2, W2.T)\n",
    "    \n",
    "    delta1[X1 <= 0] = 0\n",
    "\n",
    "    dW1 = np.dot(X.T, delta1)\n",
    "    dW1 += self.reg * W1\n",
    "    db1 = np.sum(delta1, axis=0)\n",
    "    \n",
    "    grads['W1'] = dW1\n",
    "    grads['b1'] = db1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As another sanity check, make sure you can overfit a small dataset of 50 images. First we will try a three-layer network with 100 units in each hidden layer. You will need to tweak the learning rate and initialization scale, but you should be able to overfit and achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 40) loss: 0.000000\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "axis(=1) out of bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-fa909c059b75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m                 }\n\u001b[1;32m     21\u001b[0m          )\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'o'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Anji/dlbootcamp/DLBootcamp/assignment2/cs231n/solver.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    246\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mfirst_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlast_it\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mepoch_end\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         train_acc = self.check_accuracy(self.X_train, self.y_train,\n\u001b[0;32m--> 248\u001b[0;31m                                         num_samples=1000)\n\u001b[0m\u001b[1;32m    249\u001b[0m         \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_acc_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Anji/dlbootcamp/DLBootcamp/assignment2/cs231n/solver.py\u001b[0m in \u001b[0;36mcheck_accuracy\u001b[0;34m(self, X, y, num_samples, batch_size)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m       \u001b[0my_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Anji/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m     \"\"\"\n\u001b[0;32m--> 963\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Anji/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# a downstream library like 'pandas'.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Anji/anaconda/envs/ipykernel_py2/lib/python2.7/site-packages/numpy/core/fromnumeric.pyc\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: axis(=1) out of bounds"
     ]
    }
   ],
   "source": [
    "# TODO: Use a three-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "weight_scale = 1e-2\n",
    "learning_rate = 1e-4\n",
    "model = FullyConnectedNet([100, 100],\n",
    "              weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try to use a five-layer network with 100 units on each layer to overfit 50 training examples. Again you will have to adjust the learning rate and weight initialization, but you should be able to achieve 100% training accuracy within 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: Use a five-layer Net to overfit 50 training examples.\n",
    "\n",
    "num_train = 50\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "learning_rate = 1e-3\n",
    "weight_scale = 1e-5\n",
    "model = FullyConnectedNet([100, 100, 100, 100],\n",
    "                weight_scale=weight_scale, dtype=np.float64)\n",
    "solver = Solver(model, small_data,\n",
    "                print_every=10, num_epochs=20, batch_size=25,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': learning_rate,\n",
    "                }\n",
    "         )\n",
    "solver.train()\n",
    "\n",
    "plt.plot(solver.loss_history, 'o')\n",
    "plt.title('Training loss history')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inline question: \n",
    "Did you notice anything about the comparative difficulty of training the three-layer net vs training the five layer net?\n",
    "\n",
    "# Answer:\n",
    "[FILL THIS IN]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update rules\n",
    "So far we have used vanilla stochastic gradient descent (SGD) as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGD+Momentum\n",
    "Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla stochstic gradient descent.\n",
    "\n",
    "Open the file `cs231n/optim.py` and read the documentation at the top of the file to make sure you understand the API. Implement the SGD+momentum update rule in the function `sgd_momentum` and run the following to check your implementation. You should see errors less than 1e-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from cs231n.optim import sgd_momentum\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-3, 'velocity': v}\n",
    "next_w, _ = sgd_momentum(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [ 0.1406,      0.20738947,  0.27417895,  0.34096842,  0.40775789],\n",
    "  [ 0.47454737,  0.54133684,  0.60812632,  0.67491579,  0.74170526],\n",
    "  [ 0.80849474,  0.87528421,  0.94207368,  1.00886316,  1.07565263],\n",
    "  [ 1.14244211,  1.20923158,  1.27602105,  1.34281053,  1.4096    ]])\n",
    "expected_velocity = np.asarray([\n",
    "  [ 0.5406,      0.55475789,  0.56891579, 0.58307368,  0.59723158],\n",
    "  [ 0.61138947,  0.62554737,  0.63970526,  0.65386316,  0.66802105],\n",
    "  [ 0.68217895,  0.69633684,  0.71049474,  0.72465263,  0.73881053],\n",
    "  [ 0.75296842,  0.76712632,  0.78128421,  0.79544211,  0.8096    ]])\n",
    "\n",
    "print 'next_w error: ', rel_error(next_w, expected_next_w)\n",
    "print 'velocity error: ', rel_error(expected_velocity, config['velocity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have done so, run the following to train a six-layer network with both SGD and SGD+momentum. You should see the SGD+momentum update rule converge faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "num_train = 4000\n",
    "small_data = {\n",
    "  'X_train': data['X_train'][:num_train],\n",
    "  'y_train': data['y_train'][:num_train],\n",
    "  'X_val': data['X_val'],\n",
    "  'y_val': data['y_val'],\n",
    "}\n",
    "\n",
    "solvers = {}\n",
    "\n",
    "for update_rule in ['sgd', 'sgd_momentum']:\n",
    "  print 'running with ', update_rule\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': 1e-2,\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.iteritems():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSProp and Adam\n",
    "RMSProp [1] and Adam [2] are update rules that set per-parameter learning rates by using a running average of the second moments of gradients.\n",
    "\n",
    "In the file `cs231n/optim.py`, implement the RMSProp update rule in the `rmsprop` function and implement the Adam update rule in the `adam` function, and check your implementations using the tests below.\n",
    "\n",
    "[1] Tijmen Tieleman and Geoffrey Hinton. \"Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude.\" COURSERA: Neural Networks for Machine Learning 4 (2012).\n",
    "\n",
    "[2] Diederik Kingma and Jimmy Ba, \"Adam: A Method for Stochastic Optimization\", ICLR 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test RMSProp implementation; you should see errors less than 1e-7\n",
    "from cs231n.optim import rmsprop\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "cache = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'cache': cache}\n",
    "next_w, _ = rmsprop(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.39223849, -0.34037513, -0.28849239, -0.23659121, -0.18467247],\n",
    "  [-0.132737,   -0.08078555, -0.02881884,  0.02316247,  0.07515774],\n",
    "  [ 0.12716641,  0.17918792,  0.23122175,  0.28326742,  0.33532447],\n",
    "  [ 0.38739248,  0.43947102,  0.49155973,  0.54365823,  0.59576619]])\n",
    "expected_cache = np.asarray([\n",
    "  [ 0.5976,      0.6126277,   0.6277108,   0.64284931,  0.65804321],\n",
    "  [ 0.67329252,  0.68859723,  0.70395734,  0.71937285,  0.73484377],\n",
    "  [ 0.75037008,  0.7659518,   0.78158892,  0.79728144,  0.81302936],\n",
    "  [ 0.82883269,  0.84469141,  0.86060554,  0.87657507,  0.8926    ]])\n",
    "\n",
    "print 'next_w error: ', rel_error(expected_next_w, next_w)\n",
    "print 'cache error: ', rel_error(expected_cache, config['cache'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test Adam implementation; you should see errors around 1e-7 or less\n",
    "from cs231n.optim import adam\n",
    "\n",
    "N, D = 4, 5\n",
    "w = np.linspace(-0.4, 0.6, num=N*D).reshape(N, D)\n",
    "dw = np.linspace(-0.6, 0.4, num=N*D).reshape(N, D)\n",
    "m = np.linspace(0.6, 0.9, num=N*D).reshape(N, D)\n",
    "v = np.linspace(0.7, 0.5, num=N*D).reshape(N, D)\n",
    "\n",
    "config = {'learning_rate': 1e-2, 'm': m, 'v': v, 't': 5}\n",
    "next_w, _ = adam(w, dw, config=config)\n",
    "\n",
    "expected_next_w = np.asarray([\n",
    "  [-0.40094747, -0.34836187, -0.29577703, -0.24319299, -0.19060977],\n",
    "  [-0.1380274,  -0.08544591, -0.03286534,  0.01971428,  0.0722929],\n",
    "  [ 0.1248705,   0.17744702,  0.23002243,  0.28259667,  0.33516969],\n",
    "  [ 0.38774145,  0.44031188,  0.49288093,  0.54544852,  0.59801459]])\n",
    "expected_v = np.asarray([\n",
    "  [ 0.69966,     0.68908382,  0.67851319,  0.66794809,  0.65738853,],\n",
    "  [ 0.64683452,  0.63628604,  0.6257431,   0.61520571,  0.60467385,],\n",
    "  [ 0.59414753,  0.58362676,  0.57311152,  0.56260183,  0.55209767,],\n",
    "  [ 0.54159906,  0.53110598,  0.52061845,  0.51013645,  0.49966,   ]])\n",
    "expected_m = np.asarray([\n",
    "  [ 0.48,        0.49947368,  0.51894737,  0.53842105,  0.55789474],\n",
    "  [ 0.57736842,  0.59684211,  0.61631579,  0.63578947,  0.65526316],\n",
    "  [ 0.67473684,  0.69421053,  0.71368421,  0.73315789,  0.75263158],\n",
    "  [ 0.77210526,  0.79157895,  0.81105263,  0.83052632,  0.85      ]])\n",
    "\n",
    "print 'next_w error: ', rel_error(expected_next_w, next_w)\n",
    "print 'v error: ', rel_error(expected_v, config['v'])\n",
    "print 'm error: ', rel_error(expected_m, config['m'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have debugged your RMSProp and Adam implementations, run the following to train a pair of deep networks using these new update rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "learning_rates = {'rmsprop': 1e-4, 'adam': 1e-3}\n",
    "for update_rule in ['adam', 'rmsprop']:\n",
    "  print 'running with ', update_rule\n",
    "  model = FullyConnectedNet([100, 100, 100, 100, 100], weight_scale=5e-2)\n",
    "\n",
    "  solver = Solver(model, small_data,\n",
    "                  num_epochs=5, batch_size=100,\n",
    "                  update_rule=update_rule,\n",
    "                  optim_config={\n",
    "                    'learning_rate': learning_rates[update_rule]\n",
    "                  },\n",
    "                  verbose=True)\n",
    "  solvers[update_rule] = solver\n",
    "  solver.train()\n",
    "  print\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.title('Training loss')\n",
    "plt.xlabel('Iteration')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.title('Training accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.title('Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "\n",
    "for update_rule, solver in solvers.iteritems():\n",
    "  plt.subplot(3, 1, 1)\n",
    "  plt.plot(solver.loss_history, 'o', label=update_rule)\n",
    "  \n",
    "  plt.subplot(3, 1, 2)\n",
    "  plt.plot(solver.train_acc_history, '-o', label=update_rule)\n",
    "\n",
    "  plt.subplot(3, 1, 3)\n",
    "  plt.plot(solver.val_acc_history, '-o', label=update_rule)\n",
    "  \n",
    "for i in [1, 2, 3]:\n",
    "  plt.subplot(3, 1, i)\n",
    "  plt.legend(loc='upper center', ncol=4)\n",
    "plt.gcf().set_size_inches(15, 15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a good model!\n",
    "Train the best fully-connected model that you can on CIFAR-10, storing your best model in the `best_model` variable. We require you to get at least 50% accuracy on the validation set using a fully-connected net.\n",
    "\n",
    "If you are careful it should be possible to get accuracies above 55%, but we don't require it for this part and won't assign extra credit for doing so. Later in the assignment we will ask you to train the best convolutional network that you can on CIFAR-10, and we would prefer that you spend your effort working on convolutional nets rather than fully-connected nets.\n",
    "\n",
    "You might find it useful to complete the `BatchNormalization.ipynb` and `Dropout.ipynb` notebooks before completing this part, since those techniques can help you train powerful models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "best_model = None\n",
    "################################################################################\n",
    "# TODO: Train the best FullyConnectedNet that you can on CIFAR-10. You might   #\n",
    "# batch normalization and dropout useful. Store your best model in the         #\n",
    "# best_model variable.                                                         #\n",
    "################################################################################\n",
    "pass\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test you model\n",
    "Run your best model on the validation and test sets. You should achieve above 50% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_test_pred = np.argmax(best_model.loss(X_test), axis=1)\n",
    "y_val_pred = np.argmax(best_model.loss(X_val), axis=1)\n",
    "print 'Validation set accuracy: ', (y_val_pred == y_val).mean()\n",
    "print 'Test set accuracy: ', (y_test_pred == y_test).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
